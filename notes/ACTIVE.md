# Current Task

This file is for active work. Put output and plans here.
When you complete an item, test it! then check it off here and then make a commit.

## OTel Collector CrashLoopBackOff in kenrimple-local Namespace

### Problem
Multiple `otel-collector-agent` daemonset pods are crash looping in the kenrimple-local namespace with the following panic:

```
panic: runtime error: index out of range [0] with length 0

goroutine 62 [running]:
go.opentelemetry.io/collector/pdata/pmetric.ResourceMetricsSlice.At(...)
github.com/open-telemetry/opentelemetry-collector-contrib/exporter/awss3exporter.(*s3Exporter).ConsumeMetrics(...)
	exporter.go:96 +0xde
```

### Root Cause Analysis
- The custom collector build includes `awss3exporter v0.139.0` (deploy/config-files/custom-collector/ocb-config.yaml:12)
- The S3 exporter is configured in the configmap and used in all three pipelines:
  - `logs` pipeline: exporters include `awss3`
  - `metrics` pipeline: exporters include `awss3`
  - `traces` pipeline: exporters include `awss3`
- **Bug in awss3exporter v0.139.0**: The exporter's `ConsumeMetrics` function (exporter.go:96) attempts to access `ResourceMetricsSlice.At(0)` without checking if the slice is empty
- When the collector receives an empty metrics payload (no ResourceMetrics), it panics with "index out of range [0] with length 0"

### Configuration Details
- Collector ConfigMap: `otel-collector-agent` in kenrimple-local namespace
- S3 Config:
  - Bucket: `kenrimple-devreldemo-telemetry-archive-us`
  - Region: `us-east-1`
  - Partition: `year=%Y/month=%m/day=%d/hour=%H/minute=%M`
  - Compression: gzip
  - Marshaler: otlp_proto

### Solutions (Options)

#### Option 1: Remove awss3 from metrics pipeline (Quick Fix)
Remove the S3 exporter from the metrics pipeline since that's where the empty payload is coming from:
- Keep S3 exporter for logs and traces
- Remove from metrics pipeline until exporter bug is fixed

#### Option 2: Upgrade awss3exporter version
Check if a newer version of the exporter has fixed this bug (needs research)

#### Option 3: Add filter/batch processor before S3 exporter
Add a processor to ensure empty payloads don't reach the S3 exporter

### Status
- ‚úÖ **Solution Applied: Added Batch Processor**

### Investigation Results
- Checked source code at exporter.go:96 - confirms `.At(0)` without empty check
- Latest version v0.141.0 still has the bug - upgrading won't help
- Same bug exists in ConsumeLogs() and ConsumeTraces()

### Solution Implemented
Added batch processor to metrics pipeline only (not logs/traces):

**Changes made to `skaffold-config/demo-values.yaml`:**
1. Added batch processor config (lines 53-56):
   ```yaml
   batch:
     timeout: 10s
     send_batch_size: 1024
     send_batch_max_size: 2048
   ```
2. Added batch to metrics pipeline processors (line 224)

**How this helps:**
- Batch processor accumulates metrics before sending
- Only flushes when there's actual data accumulated
- Should prevent empty metrics payloads from reaching S3 exporter
- May still forward explicit empty structs, but reduces frequency significantly

### Next Steps
1. ‚è≥ Redeploy collector to kenrimple-local namespace
2. ‚è≥ Monitor pods for crash loop resolution
3. ‚è≥ Verify S3 exports are working
4. Optional: File upstream bug report with OpenTelemetry project

## Fixed React Hydration Error in PlatformFlag Component

### Problem
The Next.js frontend was experiencing React hydration errors in production due to inconsistent client/server rendering in the `PlatformFlag` component.

### Root Cause Analysis
- **Development/Local**: Uses `envOverrides` in `demo-values.yaml` to set `ENV_PLATFORM=local` globally
- **Production**: Sets `ENV_PLATFORM=production` directly in kubernetes deployment manifest 

The hydration error occurred because:
1. Server-side rendering used `process.env.NEXT_PUBLIC_PLATFORM` (from `ENV_PLATFORM` via `next.config.js`)  
2. Client-side rendering tried to access `window.ENV.NEXT_PUBLIC_PLATFORM` (injected via script in `_document.tsx`)
3. This created a timing mismatch causing hydration errors

### Solution Applied
Fixed `src/frontend/components/PlatformFlag/PlatformFlag.tsx` to use consistent environment variable access:

**Before:**
```typescript
const { NEXT_PUBLIC_PLATFORM = 'local' } = typeof window !== 'undefined' ? window.ENV : {};
const platform = NEXT_PUBLIC_PLATFORM;
```

**After:**
```typescript
const platform = process.env.NEXT_PUBLIC_PLATFORM || 'local';
```

### Key Benefits
- ‚úÖ Eliminates server/client hydration mismatch
- ‚úÖ Uses Next.js built-in environment variable handling
- ‚úÖ Works consistently in both development and production
- ‚úÖ Simpler, more reliable code

### Files Modified
- `src/frontend/components/PlatformFlag/PlatformFlag.tsx` - Fixed environment variable access

### Testing Status
- ‚úÖ Build passes successfully
- üîÑ User will test deployment to verify fix resolves production hydration errors
